{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b6acbd9-ef1e-4853-ab9d-22d4c2713db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84af8572-1985-48f8-94bb-b3855fc575f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'Images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8880a051-33e5-4033-a9e9-b3a2fa32bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import os,shutil,math,scipy,cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rn\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix,roc_curve,auc\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import Image as pil_image\n",
    "from PIL import ImageDraw\n",
    "\n",
    "from time import time\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from IPython.display import SVG\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import layers\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.applications.vgg16 import VGG16,preprocess_input\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.nasnet import NASNetMobile\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense,Flatten,Dropout,Concatenate,GlobalAveragePooling2D,Lambda,ZeroPadding2D\n",
    "from keras.layers import SeparableConv2D,BatchNormalization,MaxPooling2D,Conv2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,TensorBoard,CSVLogger,ReduceLROnPlateau,LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21edc1f2-4a6f-4901-a337-2ad3e687f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d832d09b-ec92-4573-ad09-ca8d831e0f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = 'best_model.h5'\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    checkpoint_filepath,\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7a2557c9-ed1f-481c-8341-c76415eb19e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Breeds:\n",
      "['n02088094-Afghan_hound', 'n02106030-collie', 'n02108551-Tibetan_mastiff', 'n02094114-Norfolk_terrier', 'n02106166-Border_collie', 'n02107908-Appenzeller', 'n02106382-Bouvier_des_Flandres', 'n02091635-otterhound', 'n02108089-boxer', 'n02097209-standard_schnauzer', 'n02104365-schipperke', 'n02106662-German_shepherd', 'n02113624-toy_poodle', 'n02111500-Great_Pyrenees', 'n02101556-clumber', 'n02105505-komondor', 'n02091032-Italian_greyhound', 'n02098413-Lhasa', 'n02090721-Irish_wolfhound', 'n02096051-Airedale', 'n02099429-curly-coated_retriever', 'n02097298-Scotch_terrier', 'n02086079-Pekinese', 'n02111277-Newfoundland', 'n02093754-Border_terrier', 'n02116738-African_hunting_dog', 'n02113978-Mexican_hairless', 'n02089867-Walker_hound', 'n02102318-cocker_spaniel', 'n02089078-black-and-tan_coonhound']\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the images folder\n",
    "data_dir = 'Images'\n",
    "num_breeds = 30  # Change this to the desired number of folders\n",
    "\n",
    "# Get a list of all subdirectories (dog breed folders) in the 'Images' directory\n",
    "breed_folders = [folder for folder in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, folder))]\n",
    "\n",
    "# Choose num_breeds random folders\n",
    "selected_breeds = random.sample(breed_folders, num_breeds)\n",
    "\n",
    "# Display the selected folders\n",
    "print(\"Selected Breeds:\")\n",
    "print(selected_breeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "263fac1f-5c13-4a40-b6b5-5d344e407e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 8\n",
    "img_height, img_width = 150, 150\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0ea805cb-2e67-428f-98c1-64fde4a0295e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3519 images belonging to 30 classes.\n",
      "Found 1487 images belonging to 30 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.3,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    seed=seed,\n",
    "    classes=selected_breeds  # Pass the selected folders to the 'classes' parameter\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    seed=seed,\n",
    "    classes=selected_breeds  # Pass the selected folders to the 'classes' parameter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "789f37c0-9c06-4350-9a05-d69ee038313e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 4, 4, 512)         14714688  \n",
      "                                                                 \n",
      " global_average_pooling2d_11  (None, 512)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 30)                15390     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,730,078\n",
      "Trainable params: 7,094,814\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Freeze the convolutional base\n",
    "for layer in base_model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "for layer in base_model.layers[15:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_breeds,activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a3a0c930-434b-4007-9032-dd83dcca631f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "439/439 [==============================] - 94s 211ms/step - loss: 3.4163 - accuracy: 0.0362 - precision_22: 0.0000e+00 - recall_21: 0.0000e+00 - val_loss: 3.3831 - val_accuracy: 0.0534 - val_precision_22: 0.0000e+00 - val_recall_21: 0.0000e+00\n",
      "Epoch 2/10\n",
      "439/439 [==============================] - 60s 136ms/step - loss: 3.3856 - accuracy: 0.0544 - precision_22: 0.3333 - recall_21: 2.8482e-04 - val_loss: 3.3326 - val_accuracy: 0.0845 - val_precision_22: 0.0000e+00 - val_recall_21: 0.0000e+00\n",
      "Epoch 3/10\n",
      "439/439 [==============================] - 60s 136ms/step - loss: 3.2626 - accuracy: 0.0880 - precision_22: 0.4500 - recall_21: 0.0051 - val_loss: 3.0618 - val_accuracy: 0.1615 - val_precision_22: 0.3333 - val_recall_21: 6.7568e-04\n",
      "Epoch 4/10\n",
      "439/439 [==============================] - 61s 140ms/step - loss: 2.8707 - accuracy: 0.1834 - precision_22: 0.5516 - recall_21: 0.0396 - val_loss: 2.5086 - val_accuracy: 0.2682 - val_precision_22: 0.5957 - val_recall_21: 0.0946\n",
      "Epoch 5/10\n",
      "439/439 [==============================] - 61s 139ms/step - loss: 2.4319 - accuracy: 0.2942 - precision_22: 0.6119 - recall_21: 0.1028 - val_loss: 2.1436 - val_accuracy: 0.3838 - val_precision_22: 0.6762 - val_recall_21: 0.1595\n",
      "Epoch 6/10\n",
      "439/439 [==============================] - 60s 138ms/step - loss: 2.0321 - accuracy: 0.4130 - precision_22: 0.6753 - recall_21: 0.2079 - val_loss: 1.9357 - val_accuracy: 0.4493 - val_precision_22: 0.7442 - val_recall_21: 0.2162\n",
      "Epoch 7/10\n",
      "439/439 [==============================] - 61s 139ms/step - loss: 1.7297 - accuracy: 0.4868 - precision_22: 0.7154 - recall_21: 0.2971 - val_loss: 1.8041 - val_accuracy: 0.4709 - val_precision_22: 0.7181 - val_recall_21: 0.2703\n",
      "Epoch 8/10\n",
      "439/439 [==============================] - 62s 140ms/step - loss: 1.5629 - accuracy: 0.5275 - precision_22: 0.7449 - recall_21: 0.3526 - val_loss: 1.7224 - val_accuracy: 0.4885 - val_precision_22: 0.7094 - val_recall_21: 0.3399\n",
      "Epoch 9/10\n",
      "439/439 [==============================] - 127s 290ms/step - loss: 1.3603 - accuracy: 0.5981 - precision_22: 0.7839 - recall_21: 0.4412 - val_loss: 1.6931 - val_accuracy: 0.5203 - val_precision_22: 0.6931 - val_recall_21: 0.3601\n",
      "Epoch 10/10\n",
      "439/439 [==============================] - 72s 164ms/step - loss: 1.2010 - accuracy: 0.6443 - precision_22: 0.8046 - recall_21: 0.4962 - val_loss: 1.6622 - val_accuracy: 0.5264 - val_precision_22: 0.6800 - val_recall_21: 0.3905\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.metrics import Precision, Recall\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy', Precision(), Recall()])\n",
    "\n",
    "#model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#metrics_callback = MetricsCallback()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    "    #validation_steps = 1000,\n",
    "    #steps_per_epoch = 1000,\n",
    "    #callbacks=[metrics_callback]\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3c76bb-f5d2-44ff-81f4-b58f6c93d3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
